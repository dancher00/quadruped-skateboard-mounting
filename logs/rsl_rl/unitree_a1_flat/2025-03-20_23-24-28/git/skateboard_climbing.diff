--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   source/robot_lab/robot_lab/tasks/locomotion/velocity/config/quadruped/unitree_a1/rough_env_cfg.py
	modified:   source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/observations.py
	modified:   source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/rewards.py
	modified:   source/robot_lab/robot_lab/tasks/locomotion/velocity/velocity_env_cfg.py

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/source/robot_lab/robot_lab/tasks/locomotion/velocity/config/quadruped/unitree_a1/rough_env_cfg.py b/source/robot_lab/robot_lab/tasks/locomotion/velocity/config/quadruped/unitree_a1/rough_env_cfg.py
index 0da42eb..76a42ff 100644
--- a/source/robot_lab/robot_lab/tasks/locomotion/velocity/config/quadruped/unitree_a1/rough_env_cfg.py
+++ b/source/robot_lab/robot_lab/tasks/locomotion/velocity/config/quadruped/unitree_a1/rough_env_cfg.py
@@ -146,9 +146,10 @@ class UnitreeA1RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
         self.rewards.upward.weight = 3.0
 
         # skate rewards
-        self.rewards.skate_distance_penalty.weight = -10.0
-        self.rewards.feet_skate_contact.weight = 10.0
+        self.rewards.skate_distance_penalty.weight = 0.0
+        self.rewards.skate_feet_contact.weight = 10.0
         self.rewards.skate_rot_penalty.weight = -5.0
+        self.rewards.skate_track_lin_vel_xy_exp.weight = 3.0
 
         # If the weight of rewards is 0, set rewards to None
         if self.__class__.__name__ == "UnitreeA1RoughEnvCfg":
diff --git a/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/observations.py b/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/observations.py
index ae8de5f..57e866c 100644
--- a/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/observations.py
+++ b/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/observations.py
@@ -52,6 +52,27 @@ def skate_rot_rel(
     skate_rot_euler = torch.cat([skate_rot_euler[0].unsqueeze(1), skate_rot_euler[1].unsqueeze(1), skate_rot_euler[2].unsqueeze(1)], dim=1)
     return skate_rot_euler
 
+def target_vel(
+    env: ManagerBasedRLEnv, robot_asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), 
+    skate_asset_cfg: SceneEntityCfg = SceneEntityCfg("skateboard")
+) -> torch.Tensor:
+    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
+    # extract the used quantities (to enable type-hinting)
+    robot_asset: RigidObject = env.scene[robot_asset_cfg.name]
+    skate_asset: RigidObject = env.scene[skate_asset_cfg.name]
+
+    target_vel = skate_asset.data.root_pos_w[:, :2] - robot_asset.data.root_pos_w[:, :2]
+    norm_v = torch.norm(target_vel, dim=1, keepdim=True)
+    norm_v = norm_v.repeat(1, 2)
+    vector_norm = 1
+    unit_vector = target_vel / norm_v
+
+    # Replace elements in tensor1 with corresponding elements from tensor3 based on the condition
+    target_vel[norm_v > vector_norm] = unit_vector[norm_v > vector_norm] * vector_norm
+
+    return target_vel
+
+
 # def skate_pos_rel(
 #     env: ManagerBasedEnv,
 #     robot_asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
diff --git a/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/rewards.py b/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/rewards.py
index 407b266..3be8f79 100644
--- a/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/rewards.py
+++ b/source/robot_lab/robot_lab/tasks/locomotion/velocity/mdp/rewards.py
@@ -135,7 +135,7 @@ def skate_distance_penalty(
     reward = torch.linalg.norm(env.scene["skate_transform"].data.target_pos_source.squeeze(1), dim=1)
     return reward
 
-def feet_skate_contact(
+def skate_feet_contact(
     env: ManagerBasedRLEnv
 ) -> torch.Tensor:
     """Reward for feet contact with skateboard"""
@@ -167,6 +167,30 @@ def skate_rot_penalty(
     reward = reward * vicinity_mask
     return reward
 
+def skate_track_lin_vel_xy_exp(
+    env: ManagerBasedRLEnv, std: float, robot_asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"), 
+    skate_asset_cfg: SceneEntityCfg = SceneEntityCfg("skateboard")
+) -> torch.Tensor:
+    """Reward tracking of linear velocity commands (xy axes) using exponential kernel."""
+    # extract the used quantities (to enable type-hinting)
+    robot_asset: RigidObject = env.scene[robot_asset_cfg.name]
+    skate_asset: RigidObject = env.scene[skate_asset_cfg.name]
+
+    target_vel = skate_asset.data.root_pos_w[:, :2] - robot_asset.data.root_pos_w[:, :2]
+    norm_v = torch.norm(target_vel, dim=1, keepdim=True)
+    norm_v = norm_v.repeat(1, 2)
+    vector_norm = 1
+    unit_vector = target_vel / norm_v
+    target_vel[norm_v > vector_norm] = unit_vector[norm_v > vector_norm] * vector_norm
+
+    # compute the error
+    lin_vel_error = torch.sum(
+        torch.square(target_vel - robot_asset.data.root_lin_vel_b[:, :2]),
+        dim=1,
+    )
+    reward = torch.exp(-lin_vel_error / std**2)
+    reward *= torch.clamp(-env.scene["robot"].data.projected_gravity_b[:, 2], 0, 0.7) / 0.7
+    return reward
 
 class GaitReward(ManagerTermBase):
     """Gait enforcing reward term for quadrupeds.
diff --git a/source/robot_lab/robot_lab/tasks/locomotion/velocity/velocity_env_cfg.py b/source/robot_lab/robot_lab/tasks/locomotion/velocity/velocity_env_cfg.py
index 00ec65d..126a494 100644
--- a/source/robot_lab/robot_lab/tasks/locomotion/velocity/velocity_env_cfg.py
+++ b/source/robot_lab/robot_lab/tasks/locomotion/velocity/velocity_env_cfg.py
@@ -109,8 +109,8 @@ class MySceneCfg(InteractiveSceneCfg):
             activate_contact_sensors=False,
         ),
         init_state=ArticulationCfg.InitialStateCfg(
-            # pos=(1.0, -0.39, 0.0),
-            pos=(2.0, 2.0, 0.0),
+            pos=(1.0, -0.39, 0.0),
+            # pos=(0.0, 0.0, 0.0),
             rot=(1.0, 0.0, 0.0, 0.0),
             joint_pos={  # Указываем реальные имена шарниров из ski.usd
                 "trj0": 0.0,
@@ -142,7 +142,7 @@ class MySceneCfg(InteractiveSceneCfg):
 
     skate_transform = FrameTransformerCfg(
         prim_path="{ENV_REGEX_NS}/Robot/base",
-        source_frame_offset = OffsetCfg(pos = (0.0, 0.0, -0.35)),
+        source_frame_offset = OffsetCfg(pos = (0.0, 0.0, 0.0)),
         target_frames=[FrameTransformerCfg.FrameCfg(prim_path="{ENV_REGEX_NS}/Skateboard/base_link")],
         debug_vis=False,
     )
@@ -163,7 +163,7 @@ class CommandsCfg:
         rel_heading_envs=1.0,
         heading_command=True,
         heading_control_stiffness=0.5,
-        debug_vis=True,
+        debug_vis=False,
         ranges=mdp.UniformThresholdVelocityCommandCfg.Ranges(
             lin_vel_x=(-1.0, 1.0), lin_vel_y=(-1.0, 1.0), ang_vel_z=(-1.0, 1.0), heading=(-math.pi, math.pi)
         ),
@@ -700,15 +700,24 @@ class RewardsCfg:
         weight=0.0,
     )
 
-    feet_skate_contact = RewTerm(
-        func=mdp.feet_skate_contact,
+    skate_feet_contact = RewTerm(
+        func=mdp.skate_feet_contact,
         weight=0.0,
+        
     )    
 
     skate_rot_penalty = RewTerm(
         func=mdp.skate_rot_penalty,
         weight=0.0,
-    )    
+    )
+
+    skate_track_lin_vel_xy_exp = RewTerm(
+        func=mdp.skate_track_lin_vel_xy_exp,
+        weight=0.0,
+        params={
+            "std": math.sqrt(0.25),
+        },
+    )
 
 @configclass
 class TerminationsCfg: